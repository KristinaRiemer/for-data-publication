{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<a id='toc'></a>\n",
    "\n",
    "### Description of Notebook\n",
    "\n",
    "### I. [Import Python packages](#python_packages)\n",
    "\n",
    "### II. [Functions Used](#functions)\n",
    "\n",
    "### III. [Read in Datasets](#datasets)\n",
    "  - [Aboveground Dry Biomass](#aboveground_dry_biomass)\n",
    "  - [Canopy Height - Sensor](#canopy_height_sensor)\n",
    "  - [Canopy Height - Manual](#canopy_height_manual)\n",
    "  - [Days & GDD to Flowering](#flowering)\n",
    "  - [Days & GDD to Flag Leaf Emergence](#flag_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAC Season 4 Data Cleaning\n",
    "#### Traits\n",
    "- aboveground dry biomass\n",
    "- canopy height\n",
    "- days & growing degree days (GDD) to flowering\n",
    "- days & GDD to flag leaf emergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update July 2020\n",
    "##### The input data for the code in this notebook were downloaded from a Dryad data publication from the TERRA-REF project, lead author [David LeBauer](https://github.com/dlebauer). Information on the publication and access to data can be found in this [repository](https://github.com/terraref/data-publication). \n",
    "\n",
    "\n",
    "##### This notebook contains the code used to clean and curate sorghum data from Maricopa Agricultural Station Season Four. The input trait data were originally queried from betydb version 1 in April 2020 using this `R` code, but those data will most likely only be used for latitude and longitude values to add to the new derived datasets. \n",
    "\n",
    "```\n",
    "library(traits)\n",
    "library(dplyr)\n",
    "\n",
    "\n",
    "options(betydb_url = \"https://terraref.ncsa.illinois.edu/bety/\",\n",
    "        betydb_api_version = 'v1',\n",
    "        betydb_key = 'abcde_super_secret_key_1234')\n",
    "\n",
    "season_4 <- betydb_query(sitename  = \"~Season 4\",\n",
    "                         limit     =  \"none\")\n",
    "\n",
    "write.csv(season_4, file = 'mac_season_four_2020-04-22.csv')\n",
    "```\n",
    "- Environmental weather data were downloaded from the MAC weather station [website](https://cals.arizona.edu/azmet/06.htm). \n",
    "- Please email ejcain@arizona.edu with any questions or comments or create an issue in this [repository](https://github.com/MagicMilly/for-data-publication). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Import Python packages\n",
    "<a id='python_packages'></a>\n",
    "Return to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Functions Used\n",
    "<a id='functions'></a>\n",
    "Return to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(df, value_column, trait_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return an exploratory histogram to visualize distribution of values of specific trait.\n",
    "    \"\"\"\n",
    "    trait_name = df[trait_column].unique()[0]\n",
    "    return df[value_column].hist(color='navy').set_xlabel(trait_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_time_series(df, value_column, date_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nulls(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes dataframe as argument and returns table showing sum of null values, if any.\n",
    "    \"\"\"\n",
    "    \n",
    "    return df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes dataframe as argument and returns value counts for duplicates, if any.\n",
    "    \"\"\"\n",
    "    \n",
    "    return df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_values(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function takes a dataframe as argument and checks for number of unique values in each column.\n",
    "    Print statement will contain number of unique values, as well as the unique values for any column that\n",
    "    contains less than 5 unique values.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        \n",
    "        if df[col].nunique() < 5:\n",
    "            print(f'{df[col].nunique()} unique value(s) for {col} column: {df[col].unique()}')\n",
    "            \n",
    "        else:\n",
    "            print(f'{df[col].nunique()} values for {col} column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_range_column_values(working_df, plot_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    To assist in plot location, function takes the working dataframe name and name of plot column. \n",
    "    Range and column values are extracted from the plot name strings and added as new columns to the \n",
    "    returned dataframe. \n",
    "    \"\"\"\n",
    "    \n",
    "    new_df = working_df.copy()\n",
    "\n",
    "    new_df['range'] = new_df[plot_column].str.extract(\"Range (\\d+)\").astype(int)\n",
    "    new_df['column'] = new_df[plot_column].str.extract(\"Column (\\d+)\").astype(int)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime_column(working_df, date_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    If date column does not contain datetime objects, function takes working dataframe and name of date column\n",
    "    as arguments. The original date column is dropped, and a new dataframe with an updated datatime column\n",
    "    is returned.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_datetimes = pd.to_datetime(working_df[date_column])\n",
    "    \n",
    "    new_df_0 = working_df.drop(labels=date_column, axis=1)\n",
    "    new_df_1 = new_df_0.copy()\n",
    "    new_df_1['date'] = new_datetimes\n",
    "    \n",
    "    return new_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_value_column(working_df, value_column, trait_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes working dataframe, name of value column, and name of trait column as arguments. Returns a new dataframe\n",
    "    with the name of the trait as the new name of the value column.\n",
    "    \"\"\"\n",
    "    \n",
    "    trait = working_df[trait_column].unique()[0]\n",
    "    \n",
    "    new_df_0 = working_df.rename({value_column: trait}, axis=1)\n",
    "    new_df_1 = new_df_0.drop(labels=trait_column, axis=1)\n",
    "    \n",
    "    return new_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_blocking_height(working_df, range_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    For season 4 data, takes a working dataframe with a range column to indicate plot location within the field, \n",
    "    and returns a new dataframe with a blocking height column that will indicate if a certain range was blocked \n",
    "    by a short, medium, or tall height block.\n",
    "    \"\"\"\n",
    "    \n",
    "    short_blocks = [11, 20, 46, 50]\n",
    "    medium_blocks = [10, 12, 18, 24, 27, 29, 31, 33, 38, 51]\n",
    "    tall_blocks = [3, 4, 5, 6, 7, 8, 9, 13, 14, 15, 16, 17, 19, 21, 22, 23, 25, 26, 28, 30, 32, 34, 35, 36, 37, \n",
    "                   39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 52]\n",
    "    border = [1, 2, 53, 54]\n",
    "    \n",
    "    range_values = working_df[range_column].values\n",
    "    blocking_heights = []\n",
    "    \n",
    "    for r in range_values:\n",
    "        \n",
    "        if r in short_blocks:\n",
    "            blocking_heights.append('short')\n",
    "            \n",
    "        elif r in medium_blocks:\n",
    "            blocking_heights.append('medium')\n",
    "            \n",
    "        elif r in tall_blocks:\n",
    "            blocking_heights.append('tall')\n",
    "            \n",
    "        elif r in border:\n",
    "            blocking_heights.append('border')\n",
    "            \n",
    "        else:\n",
    "            print(f'Error with range value {r}')\n",
    "        \n",
    "    working_df_1 = working_df.copy()\n",
    "    working_df_1['blocking_height'] = blocking_heights\n",
    "    \n",
    "    return working_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_columns(working_df, new_col_order_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes working dataframe and list of new column order and returns a new dataframe with desired column order.\n",
    "    \"\"\"\n",
    "    \n",
    "    working_df_1 = pd.DataFrame(data=working_df, columns=new_col_order_list)\n",
    "    return working_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_lat_lon(lat_lon_df, lat_lon_plot_column, working_df, working_df_plot_column):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Take the dataframe with the latitude and longitude information, the name of the plot column, the working\n",
    "#     dataframe, and the name of the plot column in the working dataframe as arguments. Function will return\n",
    "#     the working dataframe with latitude and longitude values for the plots as a new dataframe.\n",
    "#     \"\"\"\n",
    "    \n",
    "# use dictionaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for later: should return True or False, to be used with strip_subplots()\n",
    "\n",
    "def check_for_subplots(df, plot_col):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function takes a dataframe and name of plot column as argument and checks for `E` or `W` subplot designations.\n",
    "    Print statement indicates the presence or lack of subplot designations.\n",
    "    \"\"\"\n",
    "\n",
    "    for name in df[plot_col].values:\n",
    "        \n",
    "        if (name.endswith(' E')) | (name.endswith(' W')):\n",
    "             return 'This dataset contains subplot designations.'\n",
    "        \n",
    "        else:\n",
    "            return 'No subplot designations.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_subplots(working_df, plot_col, new_plot_col_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function takes a dataframe, name of existing plot column, and name of new plot column as arguments.\n",
    "    If there are subplot designations present in that column, they will be stripped to return a new dataframe\n",
    "    with a plot column lacking subplot designations. This will allow for easier manipulation of the dataframe\n",
    "    and aggregate functions by full plot only.\n",
    "    \"\"\"\n",
    "    \n",
    "    plot_names = working_df[plot_col].values\n",
    "    new_plot_names = []\n",
    "    \n",
    "    for n in plot_names:\n",
    "        \n",
    "        if (n.endswith(' E') | (n.endswith(' W'))):\n",
    "            new_plot_names.append(n[:-2])\n",
    "            \n",
    "        else:\n",
    "            new_plot_names.append(n)\n",
    "            \n",
    "    working_df_1 = working_df.drop(labels=plot_col, axis=1)\n",
    "    working_df_2 = working_df_1.copy()\n",
    "    \n",
    "    working_df_2[new_plot_col_name] = new_plot_names\n",
    "    return working_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(df, name_of_dataset):\n",
    "    \n",
    "    timestamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "    output_filename = ('data/processed/' + f'{name_of_dataset}_' + f'{timestamp}.csv').replace(':', '')\n",
    "\n",
    "    df.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Read in datasets\n",
    "<a id='datasets'></a>\n",
    "Return to [Table of Contents](#toc)\n",
    "- Raw Season Four data can be downloaded from this Google [Drive](https://drive.google.com/open?id=1THk-NQYxkkej-zdQsqM7i9t-axyS0Sug)\n",
    "- Each trait - separated by method, if applicable - can be found in its own `.csv` file\n",
    "- Functions applied to all datasets\n",
    "    - Plot distribution of values\n",
    "    - Check for null values\n",
    "    - Check for duplicates\n",
    "    - Extract range and column values to add to dataframe\n",
    "    - Convert string date column values to datetime objects\n",
    "    - Rename values column (usually 'mean') to the trait being measured\n",
    "    - Add blocking height column\n",
    "- Columns dropped from all datasets\n",
    "    - `checked` \n",
    "    - `author`\n",
    "    - `season`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data queried from betydb, for latitude and longitude values of plots / sitenames\n",
    "- Slice dataset to only include unique sitename values\n",
    "- **Currently (July 2020) these values have not been added to the updated datasets, so the cells do not need to be executed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s4_0 = pd.read_csv('data/raw/mac_season_four_2020-04-22.csv', low_memory=False)\n",
    "# print(s4_0.shape)\n",
    "# s4_0.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s4_1 = s4_0[['sitename', 'lat', 'lon']]\n",
    "# print(s4_1.shape)\n",
    "# s4_1.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s4_2 = s4_1.drop_duplicates(ignore_index=True)\n",
    "# print(s4_2.shape)\n",
    "# s4_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Aboveground Dry Biomass\n",
    "<a id='aboveground_dry_biomass'></a>\n",
    "Return to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_0 = pd.read_csv('data/raw/season_4_traits/season_4_aboveground_dry_biomass_manual.csv')\n",
    "print(adb_0.shape)\n",
    "# adb_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(adb_0, 'mean', 'trait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_nulls(adb_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(adb_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_unique_values(adb_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_1 = extract_range_column_values(adb_0, 'plot')\n",
    "print(adb_1.shape)\n",
    "# adb_1.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_2 = convert_datetime_column(adb_1, 'date')\n",
    "print(adb_2.shape)\n",
    "# adb_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_3 = rename_value_column(adb_2, 'mean', 'trait')\n",
    "print(adb_3.shape)\n",
    "# adb_3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['checked', 'author', 'season']\n",
    "\n",
    "adb_4 = adb_3.drop(labels=cols_to_drop, axis=1)\n",
    "print(adb_4.shape)\n",
    "# adb_4.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_5 = add_blocking_height(adb_4, 'range')\n",
    "print(adb_5.shape)\n",
    "# adb_5.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add units (kg/ha) column to aboveground dry biomass dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_6 = adb_5.copy()\n",
    "adb_6['units'] = 'kg/ha'\n",
    "\n",
    "print(adb_6.shape)\n",
    "# adb_6.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_order = ['date', 'plot', 'range', 'column', 'scientificname', 'genotype', 'treatment', 'blocking_height', \n",
    "                 'method', 'aboveground_dry_biomass', 'units', 'method_type']\n",
    "\n",
    "adb_7 = reorder_columns(adb_6, new_col_order)\n",
    "print(adb_7.shape)\n",
    "adb_7.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dataframe to `.csv` if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(adb_7, name_of_dataset='aboveground_dry_biomass_season_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Canopy Height - Sensor\n",
    "<a id='canopy_height_sensor'></a>\n",
    "Return to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_0 = pd.read_csv('data/raw/season_4_traits/season_4_canopy_height_sensor.csv')\n",
    "print(ch_0.shape)\n",
    "# ch_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_unique_values(ch_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_nulls(ch_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(ch_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Duplicates\n",
    "\n",
    "# ch_0.loc[ch_0.duplicated() == True][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch_0.iloc[229]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch_0.loc[(ch_0.genotype == 'PI564163') & (ch_0['date'] == '2017-06-24')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_1 = ch_0.drop_duplicates(ignore_index=True)\n",
    "print(ch_1.shape)\n",
    "check_duplicates(ch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(ch_1, 'mean', 'trait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_subplots(ch_1, 'plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_2 = extract_range_column_values(ch_1, 'plot')\n",
    "print(ch_2.shape)\n",
    "# ch_2.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_3 = convert_datetime_column(ch_2, 'date')\n",
    "print(ch_3.shape)\n",
    "# ch_3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_4 = rename_value_column(ch_3, 'mean', 'trait')\n",
    "print(ch_4.shape)\n",
    "# ch_4.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_5 = add_blocking_height(ch_4, 'range')\n",
    "# ch_5.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_6 = ch_5.drop(labels=['checked', 'author', 'season'], axis=1)\n",
    "print(ch_6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add units column\n",
    "- cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_7 = ch_6.copy()\n",
    "ch_7['units'] = 'cm'\n",
    "print(ch_7.shape)\n",
    "# ch_7.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_order = ['date', 'plot', 'range', 'column', 'scientificname', 'genotype', 'treatment', 'blocking_height', \n",
    "                 'method', 'canopy_height', 'units', 'method_type']\n",
    "\n",
    "ch_8 = reorder_columns(ch_7, new_col_order)\n",
    "print(ch_8.shape)\n",
    "ch_8.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to `.csv` if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(ch_8, 'canopy_height_sensor_season_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Canopy Height - Manual\n",
    "- using SQLite for `groupby`\n",
    "\n",
    "<a id='canopy_height_manual'></a>\n",
    "Return to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm_0 = pd.read_csv('data/raw/season_4_traits/season_4_canopy_height_manual.csv')\n",
    "print(chm_0.shape)\n",
    "# chm_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(chm_0, 'mean', 'method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_nulls(chm_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(chm_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_unique_values(chm_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm_1 = extract_range_column_values(chm_0, 'plot')\n",
    "print(chm_1.shape)\n",
    "# chm_1.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm_2 = convert_datetime_column(chm_1, 'date')\n",
    "print(chm_2.shape)\n",
    "# chm_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify and Remove Subplot Designations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_subplots(chm_2, 'plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm_3 = strip_subplots(chm_2, 'plot', 'plot')\n",
    "print(chm_3.shape)\n",
    "# chm_3.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_subplots(chm_3, 'plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for plot/date/mean/treatment duplicates\n",
    "\n",
    "chm_3.duplicated(subset=['plot', 'date', 'mean', 'treatment']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect sample of duplicates\n",
    "\n",
    "# chm_3.loc[chm_3.duplicated(subset=['plot', 'date', 'mean']) == True][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chm_3.loc[(chm_3.genotype == 'PI641810') & (chm_3['mean'] == 212) & (chm_3['date'] == '2017-06-19')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicates\n",
    "\n",
    "chm_4 = chm_3.drop_duplicates(ignore_index=True, subset=['plot', 'genotype', 'treatment', 'mean', 'range', 'column',\n",
    "                                                        'date'])\n",
    "\n",
    "print(chm_4.shape)\n",
    "chm_4.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm_5 = add_blocking_height(chm_4, 'range')\n",
    "print(chm_5.shape)\n",
    "# chm_5.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use sqlite database to group by `plot`, `date`, and `mean` \n",
    "- rename `mean` to `canopy_height_cm`\n",
    "- can also drop and reorder columns at this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/interim/canopy_heights_manual_season_4.sqlite')\n",
    "cursor = conn.cursor()\n",
    "print(\"Opened database successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment next line out if db has already been created\n",
    "chm_5.to_sql('canopy_heights_manual_season_4.sqlite', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm_6 = pd.read_sql_query(\"\"\"\n",
    "                            SELECT date, plot, range, column, scientificname, genotype, treatment, blocking_height,\n",
    "                            method, ROUND(AVG([mean]), 2) AS canopy_height_cm, method_type\n",
    "                            FROM 'canopy_heights_manual_season_4.sqlite'\n",
    "                            GROUP BY plot, date,[mean]\n",
    "                            ORDER BY date ASC;\n",
    "                            \"\"\", conn)\n",
    "\n",
    "print(chm_6.shape)\n",
    "chm_6.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(chm_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dataframe to `.csv` if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(chm_6, name_of_dataset='canopy_height_manual_season_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Days & GDD to Flowering\n",
    "<a id='flowering'></a>\n",
    "Return to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need functions for days & gdd to traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_0 = pd.read_csv('data/raw/season_4_traits/season_4_flowering_time_manual.csv')\n",
    "print(fl_0.shape)\n",
    "# fl_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in updated processed weather dataset for season 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_0 = pd.read_csv('data/processed/mac_season_4_daily_weather_2020-07-01T144735.csv')\n",
    "print(weather_0.shape)\n",
    "# weather_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(fl_0, 'mean', 'trait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(fl_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_nulls(fl_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_subplots(fl_0, 'plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_unique_values(fl_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add planting date 2017-04-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_planting = datetime.date(2017,4,20)\n",
    "flower_df_1 = fl_0.copy()\n",
    "\n",
    "flower_df_1['date_of_planting'] = day_of_planting\n",
    "print(flower_df_1.shape)\n",
    "# flower_df_1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create datetime with days to flowering (`mean`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta = pd.Series([pd.Timedelta(days=i) for i in flower_df_1['mean'].values])\n",
    "dates_of_flowering = []\n",
    "\n",
    "for td in timedelta:\n",
    "    \n",
    "    date_of_flowering = day_of_planting + td\n",
    "    dates_of_flowering.append(date_of_flowering)\n",
    "    \n",
    "print(flower_df_1.shape[0])\n",
    "print(len(dates_of_flowering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_2 = flower_df_1.copy()\n",
    "flower_df_2['date_of_flowering'] = dates_of_flowering\n",
    "print(flower_df_2.shape)\n",
    "# flower_df_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add GDD to flowering dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice weather df for date and cumulative gdd values only\n",
    "\n",
    "season_4_gdd = weather_0[['date', 'gdd']]\n",
    "print(season_4_gdd.shape)\n",
    "# season_4_gdd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_4_gdd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_3 = flower_df_2.copy()\n",
    "flower_df_3.date_of_flowering = pd.to_datetime(flower_df_3.date_of_flowering)\n",
    "flower_df_3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_4_gdd_1 = season_4_gdd.copy()\n",
    "season_4_gdd_1['date'] = pd.to_datetime(season_4_gdd_1['date'])\n",
    "season_4_gdd_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_4 = flower_df_3.merge(season_4_gdd_1, how='left', left_on='date_of_flowering', right_on='date')\n",
    "print(flower_df_4.shape)\n",
    "# flower_df_4.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_5 = extract_range_column_values(flower_df_4, 'plot')\n",
    "flower_df_6 = add_blocking_height(flower_df_5, 'range')\n",
    "\n",
    "print(flower_df_6.shape)\n",
    "# flower_df_6.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_7 = rename_value_column(flower_df_6, 'mean', 'trait')\n",
    "# flower_df_7.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_8 = flower_df_7.rename({'flowering_time': 'days_to_flowering', 'gdd': 'gdd_to_flowering'}, axis=1)\n",
    "# flower_df_8.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['date_x', 'checked', 'author', 'season', 'date_of_planting', 'date_y']\n",
    "\n",
    "flower_df_9 = flower_df_8.drop(labels=cols_to_drop, axis=1)\n",
    "print(flower_df_9.shape)\n",
    "# flower_df_9.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_order = ['plot', 'range', 'column', 'scientificname', 'genotype', 'treatment', 'blocking_height', \n",
    "                 'method', 'date_of_flowering', 'days_to_flowering', 'gdd_to_flowering', 'method_type']\n",
    "\n",
    "flower_df_10 = reorder_columns(flower_df_9, new_col_order)\n",
    "print(flower_df_10.shape)\n",
    "flower_df_10.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(flower_df_10, 'days_gdd_to_flowering_season_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Days & GDD to Flag Leaf Emergence\n",
    "<a id='flag_leaf'></a>\n",
    "Return to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_0 = pd.read_csv('data/raw/season_4_traits/season_4_flag_leaf_emergence_time_manual.csv')\n",
    "print(fle_0.shape)\n",
    "# fle_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in updated processed weather dataset for season 4\n",
    "Code used to process weather data for season 4 can be found in the `season_4_weather_data_cleaning` notebook in this repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_0 = pd.read_csv('data/processed/mac_season_4_daily_weather_2020-07-01T144735.csv')\n",
    "print(weather_0.shape)\n",
    "# weather_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(fle_0, 'mean', 'trait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(fle_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_nulls(fle_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_subplots(fle_0, 'plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_unique_values(fle_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add planting date 2017-04-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_planting = datetime.date(2017,4,20)\n",
    "fle_df_1 = fle_0.copy()\n",
    "\n",
    "fle_df_1['date_of_planting'] = day_of_planting\n",
    "print(fle_df_1.shape)\n",
    "# fle_df_1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create timedelta using days to flag leaf emergence (`mean`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta = pd.Series([pd.Timedelta(days=i) for i in fle_df_1['mean'].values])\n",
    "dates_of_flag_leaf_emergence = []\n",
    "\n",
    "for td in timedelta:\n",
    "    \n",
    "    date_of_flag_leaf_emergence = day_of_planting + td\n",
    "    dates_of_flag_leaf_emergence.append(date_of_flag_leaf_emergence)\n",
    "    \n",
    "print(fle_df_1.shape[0])\n",
    "print(len(dates_of_flag_leaf_emergence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_df_2 = fle_df_1.copy()\n",
    "fle_df_2['date_of_flag_leaf_emergence'] = dates_of_flag_leaf_emergence\n",
    "print(fle_df_2.shape)\n",
    "# fle_df_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add GDD values to flag leaf emergence dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice weather df for date and cumulative gdd values only\n",
    "\n",
    "season_4_gdd = weather_0[['date', 'gdd']]\n",
    "print(season_4_gdd.shape)\n",
    "# season_4_gdd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_df_3 = fle_df_2.copy()\n",
    "fle_df_3.date_of_flag_leaf_emergence = pd.to_datetime(fle_df_3.date_of_flag_leaf_emergence)\n",
    "# fle_df_3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_4_gdd_1 = season_4_gdd.copy()\n",
    "season_4_gdd_1['date'] = pd.to_datetime(season_4_gdd_1['date'])\n",
    "season_4_gdd_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_df_4 = fle_df_3.merge(season_4_gdd_1, how='left', left_on='date_of_flag_leaf_emergence', right_on='date')\n",
    "print(fle_df_4.shape)\n",
    "# fle_df_4.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_df_5 = extract_range_column_values(fle_df_4, 'plot')\n",
    "fle_df_6 = add_blocking_height(fle_df_5, 'range')\n",
    "\n",
    "print(fle_df_6.shape)\n",
    "# fle_df_6.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_df_7 = rename_value_column(fle_df_6, 'mean', 'trait')\n",
    "# fle_df_7.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_df_8 = fle_df_7.rename({'flag_leaf_emergence_time': 'days_to_flag_leaf_emergence', 'gdd': 'gdd_to_flag_leaf_emergence'}, axis=1)\n",
    "# fle_df_8.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['date_x', 'checked', 'author', 'season', 'date_of_planting', 'date_y']\n",
    "\n",
    "fle_df_9 = fle_df_8.drop(labels=cols_to_drop, axis=1)\n",
    "print(fle_df_9.shape)\n",
    "# fle_df_9.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_order = ['plot', 'range', 'column', 'scientificname', 'genotype', 'treatment', 'blocking_height', \n",
    "                 'method', 'date_of_flag_leaf_emergence', 'days_to_flag_leaf_emergence', \n",
    "                 'gdd_to_flag_leaf_emergence', 'method_type']\n",
    "\n",
    "fle_df_10 = reorder_columns(fle_df_9, new_col_order)\n",
    "print(fle_df_10.shape)\n",
    "fle_df_10.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(fle_df_10, 'days_gdd_to_flag_leaf_emergence_season_4.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
