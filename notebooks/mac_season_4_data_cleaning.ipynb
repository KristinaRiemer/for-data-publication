{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAC Season 4 Data Cleaning\n",
    "#### Traits\n",
    "- aboveground dry biomass\n",
    "- days & growing degree days (GDD) to flowering\n",
    "- days & GDD to flag leaf emergence\n",
    "- canopy height (time series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update July 2020\n",
    "##### The input data for the code in this notebook were downloaded from a Dryad data publication from the TERRA-REF project, lead author [David LeBauer](https://github.com/dlebauer). Information on the publication and access to data can be found in this [repository](https://github.com/terraref/data-publication). \n",
    "\n",
    "\n",
    "##### This notebook contains the code used to clean and curate sorghum data from Maricopa Agricultural Station Season Four. The input trait data were originally queried from betydb version 1 in April 2020 using this `R` code, but those data are no longer used in this notebook or for the updated derived datasets. \n",
    "\n",
    "```\n",
    "library(traits)\n",
    "library(dplyr)\n",
    "\n",
    "\n",
    "options(betydb_url = \"https://terraref.ncsa.illinois.edu/bety/\",\n",
    "        betydb_api_version = 'v1',\n",
    "        betydb_key = 'abcde_super_secret_key_1234')\n",
    "\n",
    "season_4 <- betydb_query(sitename  = \"~Season 4\",\n",
    "                         limit     =  \"none\")\n",
    "\n",
    "write.csv(season_4, file = 'mac_season_four_2020-04-22.csv')\n",
    "```\n",
    "- Environmental weather data were downloaded from the MAC weather station [website](https://cals.arizona.edu/azmet/06.htm). \n",
    "- Please email ejcain@arizona.edu with any questions or comments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import sqlalchemy \n",
    "# import sqlite3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Custom Functions Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_values(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function takes a dataframe as argument and checks for number of unique values in each column.\n",
    "    Print statement will contain number of unique values, as well as the unique values for any column that\n",
    "    contains less than 5 unique values.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        \n",
    "        if df[col].nunique() < 5:\n",
    "            print(f'{df[col].nunique()} unique value(s) for {col} column: {df[col].unique()}')\n",
    "            \n",
    "        else:\n",
    "            print(f'{df[col].nunique()} values for {col} column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_range_column_values(working_df, plot_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    To assist in plot location, function takes the working dataframe name, name of the plot column, and\n",
    "    desired name of the new dataframe to be returned. Range and column values are extracted from the\n",
    "    plot name strings and added as new columns to the returned dataframe. \n",
    "    \"\"\"\n",
    "    \n",
    "    new_df = working_df.copy()\n",
    "\n",
    "    new_df['range'] = new_df[plot_column].str.extract(\"Range (\\d+)\").astype(int)\n",
    "    new_df['column'] = new_df[plot_column].str.extract(\"Column (\\d+)\").astype(int)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime_column(working_df, date_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    If date column does not contain datetime objects, function takes working dataframe and name of date column\n",
    "    as arguments. The original date column is dropped, and a new dataframe with an updated datatime column\n",
    "    is returned. \n",
    "    \"\"\"\n",
    "    \n",
    "    new_datetimes = pd.to_datetime(working_df[date_column])\n",
    "    \n",
    "    new_df_0 = working_df.drop(labels=date_column, axis=1)\n",
    "    new_df_1 = new_df_0.copy()\n",
    "    new_df_1['date'] = new_datetimes\n",
    "    \n",
    "    return new_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_value_column(working_df, value_column, trait_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes working dataframe, name of value column, and name of trait column as arguments. Returns a new dataframe\n",
    "    with the name of the trait as the new name of the value column.\n",
    "    \"\"\"\n",
    "    \n",
    "    trait = working_df[trait_column].unique()[0]\n",
    "    \n",
    "    new_df_0 = working_df.rename({value_column: trait}, axis=1)\n",
    "    new_df_1 = new_df_0.drop(labels=trait_column, axis=1)\n",
    "    \n",
    "    return new_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may no longer need this function with new data\n",
    "# def check_for_subplots(df):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Function takes a dataframe as argument and checks for sitename subplots ending in ' E' or ' W'\n",
    "#     Will return rows with subplots, if any.\n",
    "#     \"\"\"\n",
    "#     return df.loc[(df.sitename.str.endswith(' E')) | (df.sitename.str.endswith(' W'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Read in datasets\n",
    "- Season Four data can be downloaded from this Google [Drive](https://drive.google.com/open?id=1THk-NQYxkkej-zdQsqM7i9t-axyS0Sug)\n",
    "- Each trait - separated by method, if applicable - can be found in its own `.csv` file\n",
    "- Functions applied to all datasets\n",
    "    - Extract range and column values to add to dataframe\n",
    "    - Convert string date column values to datetime objects\n",
    "    - Rename values column (usually 'mean') to the trait being measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Aboveground Dry Biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_0 = pd.read_csv('data/raw/season_4_traits/season_4_aboveground_dry_biomass_manual.csv')\n",
    "print(adb_0.shape)\n",
    "# adb_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_unique_values(adb_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_1 = extract_range_column_values(adb_0, 'plot')\n",
    "print(adb_1.shape)\n",
    "# adb_1.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adb_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_2 = convert_datetime_column(adb_1, 'date')\n",
    "print(adb_2.shape)\n",
    "# adb_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adb_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_3 = rename_value_column(adb_2, 'mean', 'trait')\n",
    "print(adb_3.shape)\n",
    "# adb_3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original code to be refactored with more functions and different input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.read_csv('data/raw/mac_season_four_2020-04-22.csv', low_memory=False)\n",
    "print(df_0.shape)\n",
    "# df_0.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in df_0.columns:\n",
    "#     if df_0[col].nunique() < 5:\n",
    "#         print(f'Unique values for {col}: {df_0[col].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Unnamed: 0', 'checked', 'result_type', 'id', 'citation_id', 'site_id', 'treatment_id', 'city', \n",
    "                'scientificname', 'commonname', 'genus', 'species_id', 'author', 'citation_year', 'time', 'raw_date', \n",
    "                'month', 'year', 'dateloc', 'n', 'statname', 'stat', 'notes', 'access_level', 'entity', 'view_url', \n",
    "                'edit_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_0.drop(labels=cols_to_drop, axis=1)\n",
    "print(df_1.shape)\n",
    "# df_1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. Change `date` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dates = []\n",
    "\n",
    "for d in df_1.date.values:\n",
    "    \n",
    "    # strip '(America/Phoenix)' string from date\n",
    "    if 'Phoenix' in d:\n",
    "        new_name = d[:-18]\n",
    "        new_dates.append(new_name)\n",
    "    \n",
    "    else:\n",
    "        new_name = d\n",
    "        new_dates.append(new_name)\n",
    "        \n",
    "\n",
    "# check that length of new dates matches number of rows\n",
    "print(len(new_dates))\n",
    "print(df_1.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert string dates to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_format_dates = pd.to_datetime(new_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new column with datetime values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy df to avoid SettingWithCopyWarning\n",
    "df_2 = df_1.copy()\n",
    "df_2['date_1'] = iso_format_dates\n",
    "\n",
    "print(df_2.shape)\n",
    "# df_2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V. Extract Range & Column Values for Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_2.copy()\n",
    "\n",
    "df_3['range'] = df_3['sitename'].str.extract(\"Range (\\d+)\").astype(int)\n",
    "df_3['column'] = df_3['sitename'].str.extract(\"Column (\\d+)\").astype(int)\n",
    "\n",
    "# df_3.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI. Drop, Rename, & Reorder Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop string date column and redundant trait columns\n",
    "\n",
    "df_4 = df_3.drop(labels=['date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = df_4.rename({'date_1': 'date', 'mean': 'value'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_order = ['sitename', 'range', 'column', 'lat', 'lon', 'date', 'treatment', 'trait', 'trait_description', 'method_name', 'cultivar', 'value', 'units']\n",
    "\n",
    "df_6 = pd.DataFrame(data=df_5, columns=new_col_order).reset_index(drop=True)\n",
    "print(df_6.shape)\n",
    "# df_6.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI. Select for specific traits\n",
    "- aboveground dry biomass\n",
    "- days & GDD to flowering\n",
    "- days & GDD to flag leaf emergence\n",
    "- canopy height - time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Aboveground Dry Biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_0 = df_6.loc[df_6.trait == 'aboveground_dry_biomass']\n",
    "print(adb_0.shape)\n",
    "adb_0.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for E and W subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will have no output if there are no subplots\n",
    "\n",
    "check_for_subplots(adb_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop & Rename Columns, Set Date as Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['trait', 'trait_description', 'method_name']\n",
    "\n",
    "adb_1 = adb_0.drop(labels=cols_to_drop, axis=1)\n",
    "print(adb_1.shape)\n",
    "# adb_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_2 = adb_1.rename({'value': 'aboveground_dry_biomass'}, axis=1)\n",
    "# adb_2.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_3 = adb_2.set_index(keys='date')\n",
    "print(adb_3.shape)\n",
    "# adb_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_4 = adb_3.sort_index()\n",
    "adb_4.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write aboveground dry biomass dataframe to csv file with timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "output_filename = f'data/processed/aboveground_dry_biomass_mac_season_4_{timestamp}.csv'.replace(':', '')\n",
    "\n",
    "adb_4.to_csv(output_filename, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Days & Growing Degree Days (GDD) to Flowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_0 = df_6.loc[df_6.trait == 'flowering_time']\n",
    "print(flower_df_0.shape)\n",
    "flower_df_0.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for E and W subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will have no output if there are no subplots\n",
    "\n",
    "check_for_subplots(flower_df_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in Season Four Weather Data from MAC Weather Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_0 = pd.read_csv('data/raw/mac_weather_station_raw_daily_2017.csv')\n",
    "print(weather_df_0.shape)\n",
    "weather_df_0.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slice dataframe for season dates only and add date column\n",
    "* Planting Date: 2017-04-20, Day 110\n",
    "* Last Day of Harvest: 2017-09-16, Day 259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_1 = weather_df_0.loc[(weather_df_0.day_of_year >= 110) & (weather_df_0.day_of_year <= 259)]\n",
    "print(weather_df_1.shape)\n",
    "weather_df_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_4_date_range = pd.date_range(start='2017-04-20', end='2017-09-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_2 = weather_df_1.copy()\n",
    "weather_df_2['date'] = season_4_date_range\n",
    "print(weather_df_2.shape)\n",
    "weather_df_2.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Growing Degree Days\n",
    "- 10 degrees Celsius is base temp for sorghum\n",
    "- Daily gdd value = ((max temp + min temp) / 2) - 10 (base temp)\n",
    "- Growing Degree Days = cumulative sum of daily gdd values\n",
    "- Negative values convert to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_3 = weather_df_2.copy()\n",
    "weather_df_3['daily_gdd'] = (((weather_df_3['air_temp_max'] + weather_df_3['air_temp_min'])) / 2) - 10\n",
    "print(weather_df_3.shape)\n",
    "weather_df_3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for negative values - will return empty df if there are none\n",
    "\n",
    "weather_df_3.loc[weather_df_3.daily_gdd < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_4 = weather_df_3.copy()\n",
    "weather_df_4['gdd'] = np.rint(np.cumsum(weather_df_4['daily_gdd']))\n",
    "print(weather_df_4.shape)\n",
    "weather_df_4.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop daily gdd\n",
    "\n",
    "weather_df_5 = weather_df_4.drop(labels='daily_gdd', axis=1)\n",
    "print(weather_df_5.shape)\n",
    "# weather_df_5.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_6 = weather_df_5.copy()\n",
    "weather_df_6['cum_precip'] = np.round(np.cumsum(weather_df_6.precip_total), 2)\n",
    "weather_df_6.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Irrigation Columns\n",
    "- First water-deficit stress treatment: 2020-08-01 through 2020-08-14\n",
    "- Second water-deficit stress treatment: 2020-08-15 through 2020-08-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_df_6.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_treatment_dates = pd.date_range(start='2017-08-01', end='2017-08-14')\n",
    "second_treatment_dates = pd.date_range(start='2017-08-15', end='2017-08-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_dates = weather_df_6.date.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_treatment_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a date falls within a treatment date range, it will have a value of True in the water deficit columns\n",
    "\n",
    "first_treatment_col = []\n",
    "\n",
    "\n",
    "for d in season_dates:\n",
    "    \n",
    "    if d in first_treatment_dates:\n",
    "        \n",
    "        first_treatment_col.append(True)\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        first_treatment_col.append(False)\n",
    "        \n",
    "print(len(first_treatment_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_treatment_col = []\n",
    "\n",
    "for d in season_dates:\n",
    "    \n",
    "    if d in second_treatment_dates:\n",
    "        \n",
    "        second_treatment_col.append(True)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        second_treatment_col.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_7 = weather_df_6.copy()\n",
    "\n",
    "weather_df_7['first_water_deficit_treatment'] = first_treatment_col\n",
    "weather_df_7['second_water_deficit_treatment'] = second_treatment_col\n",
    "\n",
    "print(weather_df_7.shape)\n",
    "weather_df_7.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write weather data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "output_filename = f'data/processed/mac_season_4_daily_weather_{timestamp}.csv'.replace(':', '')\n",
    "\n",
    "weather_df_7.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add planting date 2017-04-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_planting = datetime.date(2017,4,20)\n",
    "flower_df_1 = flower_df_0.copy()\n",
    "\n",
    "flower_df_1['date_of_planting'] = day_of_planting\n",
    "print(flower_df_1.shape)\n",
    "# flower_df_1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create timedelta using days to flowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta_values = flower_df_1['value'].values\n",
    "dates_of_flowering = []\n",
    "\n",
    "for val in timedelta_values:\n",
    "    \n",
    "    date_of_flowering = day_of_planting + datetime.timedelta(days=val)\n",
    "    dates_of_flowering.append(date_of_flowering)\n",
    "    \n",
    "print(flower_df_1.shape[0])\n",
    "print(len(dates_of_flowering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_2 = flower_df_1.copy()\n",
    "flower_df_2['date_of_flowering'] = dates_of_flowering\n",
    "print(flower_df_2.shape)\n",
    "# flower_df_2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add GDD to flowering dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df for date and cumulative gdd values only\n",
    "\n",
    "season_4_gdd = weather_df_6[['date', 'gdd']]\n",
    "print(season_4_gdd.shape)\n",
    "season_4_gdd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flower_df_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_3 = flower_df_2.copy()\n",
    "flower_df_3.date_of_flowering = pd.to_datetime(flower_df_3.date_of_flowering)\n",
    "# flower_df_3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_4 = flower_df_3.merge(season_4_gdd, how='left', left_on='date_of_flowering', right_on='date')\n",
    "print(flower_df_4.shape)\n",
    "# flower_df_4.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop, Rename, and Sort Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['date_x', 'trait', 'method_name', 'units', 'date_of_planting', 'date_y']\n",
    "flower_df_5 = flower_df_4.drop(labels=cols_to_drop, axis=1)\n",
    "print(flower_df_5.shape)\n",
    "# flower_df_5.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_6 = flower_df_5.rename({'value': 'days_to_flowering', 'gdd': 'gdd_to_flowering'}, axis=1)\n",
    "print(flower_df_6.shape)\n",
    "# flower_df_6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_6.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = flower_df_6.loc[flower_df_6.duplicated() == True]\n",
    "duplicates.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates.loc[(duplicates.sitename == 'MAC Field Scanner Season 4 Range 46 Column 6') & (duplicates.cultivar == 'PI542718')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_df_7 = flower_df_6.drop_duplicates(ignore_index=True)\n",
    "print(flower_df_7.shape)\n",
    "flower_df_7.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of non-duplicates in flower_df_6: {flower_df_6.loc[flower_df_6.duplicated() == False].shape[0]}')\n",
    "print(f'Number of rows in dataframe after dropping duplicates: {flower_df_7.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write days to flowering dataframe to csv file with timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "output_filename = f'data/processed/days_gdd_to_flowering_season_4_{timestamp}.csv'.replace(':', '')\n",
    "\n",
    "flower_df_7.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Days & GDD to Flag Leaf Emergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_0 = df_6.loc[df_6.trait == 'flag_leaf_emergence_time']\n",
    "print(fle_0.shape)\n",
    "# fle_0.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for E and W subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will have no output if there are no subplots\n",
    "\n",
    "check_for_subplots(fle_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use sliced dataframe from days to flowering with date and gdd values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_4_gdd.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add planting date 2017-04-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_planting = datetime.date(2017,4,20)\n",
    "fle_1 = fle_0.copy()\n",
    "\n",
    "fle_1['date_of_planting'] = day_of_planting\n",
    "print(fle_1.shape)\n",
    "# fle_1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create timedelta using days to flag leaf emergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta_values = fle_1['value'].values\n",
    "dates_of_flag_leaf_emergence = []\n",
    "\n",
    "for val in timedelta_values:\n",
    "    \n",
    "    date_of_flag_leaf_emergence = day_of_planting + datetime.timedelta(days=val)\n",
    "    dates_of_flag_leaf_emergence.append(date_of_flag_leaf_emergence)\n",
    "    \n",
    "print(fle_1.shape[0])\n",
    "print(len(dates_of_flag_leaf_emergence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_2 = fle_1.copy()\n",
    "fle_2['date_of_flag_leaf_emergence'] = dates_of_flag_leaf_emergence\n",
    "print(fle_2.shape)\n",
    "# fle_2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add GDD to flag leaf emergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fle_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_3 = fle_2.copy()\n",
    "fle_3.date_of_flag_leaf_emergence = pd.to_datetime(fle_3.date_of_flag_leaf_emergence)\n",
    "# fle_3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_4 = fle_3.merge(season_4_gdd, how='left', left_on='date_of_flag_leaf_emergence', right_on='date')\n",
    "print(fle_4.shape)\n",
    "# fle_4.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop & Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['date_x', 'trait', 'method_name', 'units', 'date_of_planting', 'date_y']\n",
    "fle_5 = fle_4.drop(labels=cols_to_drop, axis=1)\n",
    "print(fle_5.shape)\n",
    "# fle_5.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_6 = fle_5.rename({'value': 'days_to_flag_leaf_emergence', 'gdd': 'gdd_to_flag_leaf_emergence'}, axis=1)\n",
    "print(fle_6.shape)\n",
    "# fle_6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_6.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fle_6.loc[fle_6.duplicated() == True].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fle_6.loc[(fle_6.sitename == 'MAC Field Scanner Season 4 Range 16 Column 7') & (fle_6.cultivar == 'PI152651')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fle_7 = fle_6.drop_duplicates(ignore_index=True)\n",
    "print(fle_7.shape)\n",
    "fle_7.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write days to flag leaf emergence to csv file with timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "output_filename = f'data/processed/days_gdd_to_flag_leaf_emergence_season_4_{timestamp}.csv'.replace(':', '')\n",
    "\n",
    "fle_7.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Canopy Height - Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_0 = df_6.loc[df_6.trait == 'canopy_height']\n",
    "print(ch_0.shape)\n",
    "# ch_0.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplots = check_for_subplots(ch_0)\n",
    "subplots.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take average canopy height values for subplots on same day\n",
    "- Strip ` E` and ` W` subplot designations\n",
    "- Group by rows with the same sitename and date and take the average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sitename_values = ch_0.sitename.values\n",
    "no_e_w_names = []\n",
    "\n",
    "for name in sitename_values:\n",
    "    \n",
    "    if name.endswith(' W') | name.endswith(' E'):\n",
    "        name = name[:-2]\n",
    "        no_e_w_names.append(name)\n",
    "        \n",
    "    else:\n",
    "        no_e_w_names.append(name)\n",
    "        \n",
    "print(len(no_e_w_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new sitename column with no subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_1 = ch_0.copy()\n",
    "ch_1['sitename_1'] = no_e_w_names\n",
    "print(ch_1.shape)\n",
    "# ch_1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use sqlite database to group by `sitename_1` and `date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('canopy_heights_season_4.sqlite')\n",
    "cursor = conn.cursor()\n",
    "print(\"Opened database successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment next line out if db has already been created\n",
    "ch_1.to_sql('canopy_heights_season_4.sqlite', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_2 = pd.read_sql_query(\"\"\"\n",
    "                            SELECT sitename_1 AS sitename, range, column, lat, lon, date, treatment, \n",
    "                            trait, trait_description, method_name, cultivar, \n",
    "                            ROUND(AVG(value), 2) AS avg_canopy_height, units \n",
    "                            FROM 'canopy_heights_season_4.sqlite'\n",
    "                            GROUP BY sitename_1, date, cultivar\n",
    "                            ORDER BY date ASC;\n",
    "                            \"\"\", conn)\n",
    "\n",
    "print(ch_2.shape)\n",
    "# ch_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "\n",
    "sample_with_subplot = ch_1.loc[(ch_1.range == 5) & (ch_1.column == 7) & (ch_1.date == '2017-07-11')]\n",
    "sample_with_subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check - should have only one row for the above group\n",
    "\n",
    "sample_without_subplot = ch_2.loc[(ch_2.range == 5) & (ch_2.column == 7) & (ch_2.date == '2017-07-11 00:00:00')]\n",
    "sample_without_subplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop, Rename, and Reorder Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['trait', 'trait_description', 'units']\n",
    "\n",
    "ch_3 = ch_2.drop(labels=cols_to_drop, axis=1)\n",
    "print(ch_3.shape)\n",
    "# ch_3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_4 = ch_3.rename({'avg_canopy_height': 'canopy_height_cm'}, axis=1)\n",
    "# ch_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_5 = ch_4.set_index(keys='date').sort_index()\n",
    "print(ch_5.shape)\n",
    "ch_5.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write canopy height dataframe to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "output_filename = f'data/processed/canopy_heights_season_4_{timestamp}.csv'.replace(':', '')\n",
    "\n",
    "ch_5.to_csv(output_filename, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
