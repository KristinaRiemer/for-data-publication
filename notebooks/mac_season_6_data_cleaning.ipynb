{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<a id='toc'></a>\n",
    "\n",
    "### Description of Notebook\n",
    "\n",
    "### I. [Import Python packages](#python_packages)\n",
    "\n",
    "### II. [Functions Used](#functions)\n",
    "\n",
    "### III. [Read in Datasets](#datasets)\n",
    "  - [Aboveground Dry Biomass](#aboveground_dry_biomass)\n",
    "  - [Canopy Height - Sensor](#canopy_height_sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAC Season 6 Data Cleaning\n",
    "\n",
    "#### Season Dates\n",
    "- Planting: 2018-04-25\n",
    "- Last Day of Harvest: 2018-08-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data & Code\n",
    "\n",
    "- This notebook contains the code used to clean Season 6 Sorghum Data from the Maricopa Agricultural Center (MAC). Information about and access to the input data can be found in this Dryad data publication [repository](https://github.com/terraref/data-publication).\n",
    "- The input data were originally queried and downloaded using this `R` code:\n",
    "\n",
    "```\n",
    "library(traits)\n",
    "\n",
    "options(betydb_url = \"https://terraref.ncsa.illinois.edu/bety/\",\n",
    "        betydb_api_version = 'v1',\n",
    "        betydb_key = 'abcde_super_secret_key_1234')\n",
    "\n",
    "season_6 <- betydb_query(sitename  = \"~Season 6\",\n",
    "                         limit     =  \"none\")\n",
    "\n",
    "write.csv(season_6, file = 'mac_season_six_2020-04-22.csv')\n",
    "```\n",
    "\n",
    "- Environmental data for 2018 can be downloaded from the MAC weather station [website](https://cals.arizona.edu/azmet/06.htm). \n",
    "- Please email ejcain@arizona.edu with any questions or comments, or submit an issue to this GitHub [repository](https://github.com/MagicMilly/for-data-publication).\n",
    "\n",
    "#### Plot design and blocking information can be found [here](https://terraref.ncsa.illinois.edu/bety/api/v1/experiments?name=~MAC+Season+6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Import Python packages\n",
    "<a id='python_packages'></a>\n",
    "Return to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Functions Used\n",
    "<a id='functions'></a>\n",
    "Return to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(df, value_column, trait_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return an exploratory histogram to visualize distribution of values of specific trait.\n",
    "    \"\"\"\n",
    "    trait_name = df[trait_column].unique()[0]\n",
    "    return df[value_column].hist(color='navy').set_xlabel(trait_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_time_series(df, value_column, date_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nulls(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes dataframe as argument and returns table showing sum of null values, if any.\n",
    "    \"\"\"\n",
    "    \n",
    "    return df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes dataframe as argument and returns value counts for duplicates, if any.\n",
    "    \"\"\"\n",
    "    \n",
    "    return df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_values(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function takes a dataframe as argument and checks for number of unique values in each column.\n",
    "    Print statement will contain number of unique values, as well as the unique values for any column that\n",
    "    contains less than 5 unique values.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        \n",
    "        if df[col].nunique() < 5:\n",
    "            print(f'{df[col].nunique()} unique value(s) for {col} column: {df[col].unique()}')\n",
    "            \n",
    "        else:\n",
    "            print(f'{df[col].nunique()} values for {col} column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_range_column_values(working_df, plot_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    To assist in plot location, function takes the working dataframe name and name of plot column. \n",
    "    Range and column values are extracted from the plot name strings and added as new columns to the \n",
    "    returned dataframe. \n",
    "    \"\"\"\n",
    "    \n",
    "    new_df = working_df.copy()\n",
    "\n",
    "    new_df['range'] = new_df[plot_column].str.extract(\"Range (\\d+)\").astype(int)\n",
    "    new_df['column'] = new_df[plot_column].str.extract(\"Column (\\d+)\").astype(int)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime_column(working_df, date_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    If date column does not contain datetime objects, function takes working dataframe and name of date column\n",
    "    as arguments. The original date column is dropped, and a new dataframe with an updated datatime column\n",
    "    is returned.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_datetimes = pd.to_datetime(working_df[date_column])\n",
    "    \n",
    "    new_df_0 = working_df.drop(labels=date_column, axis=1)\n",
    "    new_df_1 = new_df_0.copy()\n",
    "    new_df_1['date'] = new_datetimes\n",
    "    \n",
    "    return new_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_value_column(working_df, value_column, trait_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes working dataframe, name of value column, and name of trait column as arguments. Returns a new dataframe\n",
    "    with the name of the trait as the new name of the value column.\n",
    "    \"\"\"\n",
    "    \n",
    "    trait = working_df[trait_column].unique()[0]\n",
    "    \n",
    "    new_df_0 = working_df.rename({value_column: trait}, axis=1)\n",
    "    new_df_1 = new_df_0.drop(labels=trait_column, axis=1)\n",
    "    \n",
    "    return new_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_columns(working_df, new_col_order_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes working dataframe and list of new column order and returns a new dataframe with desired column order.\n",
    "    \"\"\"\n",
    "    \n",
    "    working_df_1 = pd.DataFrame(data=working_df, columns=new_col_order_list)\n",
    "    return working_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_lat_lon(lat_lon_df, lat_lon_plot_column, working_df, working_df_plot_column):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Take the dataframe with the latitude and longitude information, the name of the plot column, the working\n",
    "#     dataframe, and the name of the plot column in the working dataframe as arguments. Function will return\n",
    "#     the working dataframe with latitude and longitude values for the plots as a new dataframe.\n",
    "#     \"\"\"\n",
    "    \n",
    "# use dictionaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_subplots(df, plot_col):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function takes a dataframe and name of plot column as argument and checks for `E` or `W` subplot designations.\n",
    "    Print statement indicates the presence or lack of subplot designations.\n",
    "    \"\"\"\n",
    "\n",
    "    for name in df[plot_col].values:\n",
    "        \n",
    "        if (name.endswith(' E')) | (name.endswith(' W')):\n",
    "             return 'This dataset contains subplot designations.'\n",
    "        \n",
    "        else:\n",
    "            return 'No subplot designations.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(df, name_of_dataset):\n",
    "    \n",
    "    timestamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "    output_filename = ('data/processed/' + f'{name_of_dataset}_' + f'{timestamp}.csv').replace(':', '')\n",
    "\n",
    "    df.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Read in datasets\n",
    "<a id='datasets'></a>\n",
    "Return to [Table of Contents](#toc)\n",
    "- Raw Season Six data can be downloaded from this Google [Drive](https://drive.google.com/open?id=1THk-NQYxkkej-zdQsqM7i9t-axyS0Sug)\n",
    "- Each trait - separated by method, if applicable - can be found in its own `.csv` file\n",
    "- Functions applied to all datasets\n",
    "    - Plot distribution of values\n",
    "    - Check for null values\n",
    "    - Check for duplicates\n",
    "    - Extract range and column values to add to dataframe\n",
    "    - Convert string date column values to datetime objects\n",
    "    - Rename values column (usually 'mean') to the trait being measured\n",
    "- Columns dropped from all datasets\n",
    "    - `checked` \n",
    "    - `author`\n",
    "    - `season`\n",
    "    - `treatment`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Aboveground Dry Biomass\n",
    "<a id='aboveground_dry_biomass'></a>\n",
    "Return to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_0 = pd.read_csv('data/raw/season_6_traits/season_6_aboveground_dry_biomass_manual.csv')\n",
    "print(adb_0.shape)\n",
    "# adb_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(adb_0, 'mean', 'trait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_nulls(adb_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(adb_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_unique_values(adb_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_1 = extract_range_column_values(adb_0, 'plot')\n",
    "print(adb_1.shape)\n",
    "# adb_1.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_2 = convert_datetime_column(adb_1, 'date')\n",
    "print(adb_2.shape)\n",
    "# adb_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_3 = rename_value_column(adb_2, 'mean', 'trait')\n",
    "print(adb_3.shape)\n",
    "# adb_3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['checked', 'author', 'season', 'treatment']\n",
    "\n",
    "adb_4 = adb_3.drop(labels=cols_to_drop, axis=1)\n",
    "print(adb_4.shape)\n",
    "# adb_4.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add units (kg/ha) column to aboveground dry biomass dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_5 = adb_4.copy()\n",
    "adb_5['units'] = 'kg/ha'\n",
    "\n",
    "print(adb_5.shape)\n",
    "# adb_5.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_order = ['date', 'plot', 'range', 'column', 'scientificname', 'genotype', 'method', \n",
    "                 'aboveground_dry_biomass', 'units', 'method_type']\n",
    "\n",
    "adb_6 = reorder_columns(adb_5, new_col_order)\n",
    "print(adb_6.shape)\n",
    "adb_6.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dataframe to `.csv` if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(adb_6, name_of_dataset='aboveground_dry_biomass_season_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Canopy Height - Sensor\n",
    "<a id='canopy_height_sensor'></a>\n",
    "Return to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_0 = pd.read_csv('data/raw/season_6_traits/season_6_canopy_height_sensor.csv')\n",
    "print(ch_0.shape)\n",
    "# ch_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_unique_values(ch_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_nulls(ch_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_subplots(ch_0, 'plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(ch_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Duplicates\n",
    "\n",
    "# ch_0.loc[ch_0.duplicated() == True][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch_0.iloc[1473]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch_0.loc[(ch_0.genotype == 'PI179749') & (ch_0['date'] == '2018-07-20') & (ch_0['mean'] == 350)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_1 = ch_0.drop_duplicates(ignore_index=True)\n",
    "print(ch_1.shape)\n",
    "check_duplicates(ch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_hist(ch_1, 'mean', 'trait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_2 = extract_range_column_values(ch_1, 'plot')\n",
    "print(ch_2.shape)\n",
    "# ch_2.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_3 = convert_datetime_column(ch_2, 'date')\n",
    "print(ch_3.shape)\n",
    "# ch_3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_4 = rename_value_column(ch_3, 'mean', 'trait')\n",
    "print(ch_4.shape)\n",
    "# ch_4.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add units (cm) to column name\n",
    "\n",
    "ch_5 = ch_4.rename({'canopy_height': 'canopy_height_cm'}, axis=1)\n",
    "# ch_5.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_6 = ch_5.drop(labels=['checked', 'author', 'season', 'treatment'], axis=1)\n",
    "print(ch_6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_order = ['date', 'plot', 'range', 'column', 'scientificname', 'genotype', 'method', 'canopy_height_cm',\n",
    "                 'method_type']\n",
    "\n",
    "ch_7 = reorder_columns(ch_6, new_col_order)\n",
    "print(ch_7.shape)\n",
    "ch_7.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to `.csv` if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(ch_7, 'canopy_height_season_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
